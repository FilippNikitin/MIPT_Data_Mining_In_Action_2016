{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "with open('./cifar10/cifar-10-batches-py/data_batch_1', 'rb') as f:\n",
    "    datadict = pickle.load(f)\n",
    "    X = datadict['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cifar import load_CIFAR10\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "\n",
    "cifar10_dir = './cifar10/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8').reshape(32, 32, 3))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,3,32,32)\n",
    "X_test = X_test.reshape(-1,3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release.  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 89.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from theano import tensor as T\n",
    "from lasagne.nonlinearities import *\n",
    "\n",
    "input_X = T.tensor4(\"X\", dtype = 'float32')\n",
    "target_y = T.vector(\"target Y integer\",dtype='int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так задаётся архитектура нейронки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 32, 32)\n",
      "(None, 64, 32, 32)\n",
      "(None, 64, 16, 16)\n",
      "(None, 128, 16, 16)\n",
      "(None, 128, 8, 8)\n",
      "(None, 256, 8, 8)\n",
      "(None, 256, 4, 4)\n",
      "(None, 512, 4, 4)\n",
      "(None, 512, 2, 2)\n",
      "(None, 1024, 2, 2)\n",
      "(None, 1024, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import Conv2DLayer\n",
    "from lasagne.layers import MaxPool2DLayer\n",
    "input_layer = lasagne.layers.InputLayer(shape=(None,3, 32, 32), input_var=input_X)\n",
    "#nnet = <сверочная нейросеть>\n",
    "# Для вдохновления \n",
    "# - http://torch.ch/blog/2015/07/30/cifar.html\n",
    "# - http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "# - https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n",
    "nnet = lasagne.layers.Conv2DLayer(    \n",
    "            input_layer, num_filters=64,\n",
    "            filter_size=(5, 5), pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "print(nnet.output_shape)\n",
    "\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=64,\n",
    "            filter_size=(5, 5),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "print(nnet.output_shape)\n",
    "\n",
    "nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "print(nnet.output_shape)\n",
    "\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=128,\n",
    "            filter_size=(5, 5),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=128,\n",
    "            filter_size=(5, 5),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "print(nnet.output_shape)\n",
    "nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "print(nnet.output_shape)\n",
    "\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=256,\n",
    "            filter_size=(5, 5),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=256,\n",
    "            filter_size=(5, 5),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "print(nnet.output_shape)\n",
    "nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "print(nnet.output_shape)\n",
    "\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=512,\n",
    "            filter_size=(3, 3),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=512,\n",
    "            filter_size=(3, 3),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "print(nnet.output_shape)\n",
    "nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "print(nnet.output_shape)\n",
    "\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=1024,\n",
    "            filter_size=(1, 1),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "nnet = lasagne.layers.Conv2DLayer(\n",
    "            nnet, num_filters=1024,\n",
    "            filter_size=(1, 1),pad='same',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(gain='relu')\n",
    "            )\n",
    "print(nnet.output_shape)\n",
    "nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "print(nnet.output_shape)\n",
    "\n",
    "dense_output = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(nnet, p=.5),\n",
    "        num_units=10,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#предсказание нейронки (theano-преобразование)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#все веса нейронки (shared-переменные)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### дальше вы могли бы просто\n",
    "* задать функцию ошибки вручную\n",
    "* посчитать градиент ошибки по all_weights\n",
    "* написать updates\n",
    "* но это долго, а простой шаг по градиенту - не самый лучший смособ оптимизировать веса\n",
    "\n",
    "Вместо этого, опять используем lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted, target_y).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "updates = lasagne.updates.adam(loss, all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([input_X,target_y],[loss, accuracy], updates= updates, allow_input_downcast=True )\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вот и всё, пошли её учить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 58 took 283.871s\n",
      "  training loss (in-iteration):\t\t1.634891\n",
      "  train accuracy:\t\t39.62 %\n",
      "  validation accuracy:\t\t42.84 %\n",
      "Epoch 2 of 58 took 282.479s\n",
      "  training loss (in-iteration):\t\t1.541642\n",
      "  train accuracy:\t\t43.94 %\n",
      "  validation accuracy:\t\t43.03 %\n",
      "Epoch 3 of 58 took 283.167s\n",
      "  training loss (in-iteration):\t\t1.479293\n",
      "  train accuracy:\t\t46.24 %\n",
      "  validation accuracy:\t\t45.10 %\n",
      "Epoch 4 of 58 took 282.548s\n",
      "  training loss (in-iteration):\t\t1.433143\n",
      "  train accuracy:\t\t48.31 %\n",
      "  validation accuracy:\t\t45.86 %\n",
      "Epoch 5 of 58 took 281.488s\n",
      "  training loss (in-iteration):\t\t1.382827\n",
      "  train accuracy:\t\t50.24 %\n",
      "  validation accuracy:\t\t47.47 %\n",
      "Epoch 6 of 58 took 281.291s\n",
      "  training loss (in-iteration):\t\t1.351014\n",
      "  train accuracy:\t\t51.60 %\n",
      "  validation accuracy:\t\t47.20 %\n",
      "Epoch 7 of 58 took 281.258s\n",
      "  training loss (in-iteration):\t\t1.317643\n",
      "  train accuracy:\t\t52.80 %\n",
      "  validation accuracy:\t\t47.22 %\n",
      "Epoch 8 of 58 took 281.203s\n",
      "  training loss (in-iteration):\t\t1.295277\n",
      "  train accuracy:\t\t53.47 %\n",
      "  validation accuracy:\t\t46.56 %\n",
      "Epoch 9 of 58 took 281.125s\n",
      "  training loss (in-iteration):\t\t1.250852\n",
      "  train accuracy:\t\t55.27 %\n",
      "  validation accuracy:\t\t46.73 %\n",
      "Epoch 10 of 58 took 281.085s\n",
      "  training loss (in-iteration):\t\t1.237183\n",
      "  train accuracy:\t\t55.70 %\n",
      "  validation accuracy:\t\t46.83 %\n",
      "Epoch 11 of 58 took 281.019s\n",
      "  training loss (in-iteration):\t\t1.201136\n",
      "  train accuracy:\t\t57.30 %\n",
      "  validation accuracy:\t\t47.41 %\n",
      "Epoch 12 of 58 took 280.986s\n",
      "  training loss (in-iteration):\t\t1.184708\n",
      "  train accuracy:\t\t57.45 %\n",
      "  validation accuracy:\t\t47.00 %\n",
      "Epoch 13 of 58 took 281.007s\n",
      "  training loss (in-iteration):\t\t1.171499\n",
      "  train accuracy:\t\t58.25 %\n",
      "  validation accuracy:\t\t48.17 %\n",
      "Epoch 14 of 58 took 281.004s\n",
      "  training loss (in-iteration):\t\t1.160232\n",
      "  train accuracy:\t\t58.55 %\n",
      "  validation accuracy:\t\t47.20 %\n",
      "Epoch 15 of 58 took 281.021s\n",
      "  training loss (in-iteration):\t\t1.143988\n",
      "  train accuracy:\t\t59.29 %\n",
      "  validation accuracy:\t\t47.92 %\n",
      "Epoch 16 of 58 took 281.018s\n",
      "  training loss (in-iteration):\t\t1.127820\n",
      "  train accuracy:\t\t59.69 %\n",
      "  validation accuracy:\t\t48.01 %\n",
      "Epoch 17 of 58 took 281.023s\n",
      "  training loss (in-iteration):\t\t1.120032\n",
      "  train accuracy:\t\t60.13 %\n",
      "  validation accuracy:\t\t47.67 %\n",
      "Epoch 18 of 58 took 281.051s\n",
      "  training loss (in-iteration):\t\t1.101582\n",
      "  train accuracy:\t\t60.94 %\n",
      "  validation accuracy:\t\t47.65 %\n",
      "Epoch 19 of 58 took 280.968s\n",
      "  training loss (in-iteration):\t\t1.084058\n",
      "  train accuracy:\t\t61.44 %\n",
      "  validation accuracy:\t\t46.10 %\n",
      "Epoch 20 of 58 took 281.100s\n",
      "  training loss (in-iteration):\t\t1.088196\n",
      "  train accuracy:\t\t61.50 %\n",
      "  validation accuracy:\t\t46.66 %\n",
      "Epoch 21 of 58 took 281.018s\n",
      "  training loss (in-iteration):\t\t1.066803\n",
      "  train accuracy:\t\t62.21 %\n",
      "  validation accuracy:\t\t47.18 %\n",
      "Epoch 22 of 58 took 280.970s\n",
      "  training loss (in-iteration):\t\t1.046832\n",
      "  train accuracy:\t\t63.08 %\n",
      "  validation accuracy:\t\t47.60 %\n",
      "Epoch 23 of 58 took 280.942s\n",
      "  training loss (in-iteration):\t\t1.045667\n",
      "  train accuracy:\t\t62.89 %\n",
      "  validation accuracy:\t\t47.38 %\n",
      "Epoch 24 of 58 took 280.896s\n",
      "  training loss (in-iteration):\t\t1.034445\n",
      "  train accuracy:\t\t63.47 %\n",
      "  validation accuracy:\t\t46.48 %\n",
      "Epoch 25 of 58 took 280.882s\n",
      "  training loss (in-iteration):\t\t1.007664\n",
      "  train accuracy:\t\t64.62 %\n",
      "  validation accuracy:\t\t47.12 %\n",
      "Epoch 26 of 58 took 280.863s\n",
      "  training loss (in-iteration):\t\t0.987677\n",
      "  train accuracy:\t\t65.47 %\n",
      "  validation accuracy:\t\t47.71 %\n",
      "Epoch 27 of 58 took 280.931s\n",
      "  training loss (in-iteration):\t\t0.994866\n",
      "  train accuracy:\t\t65.48 %\n",
      "  validation accuracy:\t\t47.49 %\n",
      "Epoch 28 of 58 took 280.908s\n",
      "  training loss (in-iteration):\t\t0.952099\n",
      "  train accuracy:\t\t66.64 %\n",
      "  validation accuracy:\t\t47.53 %\n",
      "Epoch 29 of 58 took 280.945s\n",
      "  training loss (in-iteration):\t\t0.935099\n",
      "  train accuracy:\t\t67.37 %\n",
      "  validation accuracy:\t\t47.46 %\n",
      "Epoch 30 of 58 took 280.927s\n",
      "  training loss (in-iteration):\t\t0.922259\n",
      "  train accuracy:\t\t67.97 %\n",
      "  validation accuracy:\t\t45.84 %\n",
      "Epoch 31 of 58 took 280.950s\n",
      "  training loss (in-iteration):\t\t0.883044\n",
      "  train accuracy:\t\t69.27 %\n",
      "  validation accuracy:\t\t46.71 %\n",
      "Epoch 32 of 58 took 280.877s\n",
      "  training loss (in-iteration):\t\t0.845719\n",
      "  train accuracy:\t\t70.96 %\n",
      "  validation accuracy:\t\t47.45 %\n",
      "Epoch 33 of 58 took 280.998s\n",
      "  training loss (in-iteration):\t\t0.840049\n",
      "  train accuracy:\t\t71.01 %\n",
      "  validation accuracy:\t\t47.53 %\n",
      "Epoch 34 of 58 took 281.018s\n",
      "  training loss (in-iteration):\t\t0.826725\n",
      "  train accuracy:\t\t71.52 %\n",
      "  validation accuracy:\t\t46.73 %\n",
      "Epoch 35 of 58 took 280.834s\n",
      "  training loss (in-iteration):\t\t0.776538\n",
      "  train accuracy:\t\t73.46 %\n",
      "  validation accuracy:\t\t46.68 %\n",
      "Epoch 36 of 58 took 280.860s\n",
      "  training loss (in-iteration):\t\t0.746753\n",
      "  train accuracy:\t\t74.55 %\n",
      "  validation accuracy:\t\t46.20 %\n",
      "Epoch 37 of 58 took 280.890s\n",
      "  training loss (in-iteration):\t\t0.718988\n",
      "  train accuracy:\t\t75.45 %\n",
      "  validation accuracy:\t\t46.13 %\n",
      "Epoch 38 of 58 took 281.035s\n",
      "  training loss (in-iteration):\t\t0.676670\n",
      "  train accuracy:\t\t77.11 %\n",
      "  validation accuracy:\t\t47.02 %\n",
      "Epoch 39 of 58 took 280.842s\n",
      "  training loss (in-iteration):\t\t0.641855\n",
      "  train accuracy:\t\t78.52 %\n",
      "  validation accuracy:\t\t47.68 %\n",
      "Epoch 40 of 58 took 280.770s\n",
      "  training loss (in-iteration):\t\t0.607697\n",
      "  train accuracy:\t\t79.69 %\n",
      "  validation accuracy:\t\t47.98 %\n",
      "Epoch 41 of 58 took 280.806s\n",
      "  training loss (in-iteration):\t\t0.599723\n",
      "  train accuracy:\t\t80.03 %\n",
      "  validation accuracy:\t\t47.36 %\n",
      "Epoch 42 of 58 took 280.941s\n",
      "  training loss (in-iteration):\t\t0.609425\n",
      "  train accuracy:\t\t79.77 %\n",
      "  validation accuracy:\t\t47.18 %\n",
      "Epoch 43 of 58 took 280.951s\n",
      "  training loss (in-iteration):\t\t0.569471\n",
      "  train accuracy:\t\t81.36 %\n",
      "  validation accuracy:\t\t46.51 %\n",
      "Epoch 44 of 58 took 280.993s\n",
      "  training loss (in-iteration):\t\t0.578728\n",
      "  train accuracy:\t\t81.13 %\n",
      "  validation accuracy:\t\t47.01 %\n",
      "Epoch 45 of 58 took 281.014s\n",
      "  training loss (in-iteration):\t\t0.496842\n",
      "  train accuracy:\t\t83.82 %\n",
      "  validation accuracy:\t\t47.68 %\n",
      "Epoch 46 of 58 took 280.998s\n",
      "  training loss (in-iteration):\t\t0.486654\n",
      "  train accuracy:\t\t84.32 %\n",
      "  validation accuracy:\t\t47.33 %\n",
      "Epoch 47 of 58 took 280.956s\n",
      "  training loss (in-iteration):\t\t0.496529\n",
      "  train accuracy:\t\t84.15 %\n",
      "  validation accuracy:\t\t47.04 %\n",
      "Epoch 48 of 58 took 280.861s\n",
      "  training loss (in-iteration):\t\t0.465761\n",
      "  train accuracy:\t\t85.17 %\n",
      "  validation accuracy:\t\t47.79 %\n",
      "Epoch 49 of 58 took 281.178s\n",
      "  training loss (in-iteration):\t\t0.441808\n",
      "  train accuracy:\t\t86.10 %\n",
      "  validation accuracy:\t\t46.77 %\n",
      "Epoch 50 of 58 took 281.228s\n",
      "  training loss (in-iteration):\t\t0.392098\n",
      "  train accuracy:\t\t87.57 %\n",
      "  validation accuracy:\t\t47.17 %\n",
      "Epoch 51 of 58 took 280.827s\n",
      "  training loss (in-iteration):\t\t0.403362\n",
      "  train accuracy:\t\t87.37 %\n",
      "  validation accuracy:\t\t46.62 %\n",
      "Epoch 52 of 58 took 280.980s\n",
      "  training loss (in-iteration):\t\t0.358436\n",
      "  train accuracy:\t\t88.98 %\n",
      "  validation accuracy:\t\t46.68 %\n",
      "Epoch 53 of 58 took 280.852s\n",
      "  training loss (in-iteration):\t\t0.377122\n",
      "  train accuracy:\t\t88.34 %\n",
      "  validation accuracy:\t\t46.71 %\n",
      "Epoch 54 of 58 took 281.186s\n",
      "  training loss (in-iteration):\t\t0.360896\n",
      "  train accuracy:\t\t88.72 %\n",
      "  validation accuracy:\t\t46.65 %\n",
      "Epoch 55 of 58 took 280.962s\n",
      "  training loss (in-iteration):\t\t0.354785\n",
      "  train accuracy:\t\t89.09 %\n",
      "  validation accuracy:\t\t47.01 %\n",
      "Epoch 56 of 58 took 280.965s\n",
      "  training loss (in-iteration):\t\t0.321117\n",
      "  train accuracy:\t\t90.12 %\n",
      "  validation accuracy:\t\t46.44 %\n",
      "Epoch 57 of 58 took 280.920s\n",
      "  training loss (in-iteration):\t\t0.321879\n",
      "  train accuracy:\t\t90.43 %\n",
      "  validation accuracy:\t\t47.09 %\n",
      "Epoch 58 of 58 took 280.981s\n",
      "  training loss (in-iteration):\t\t0.332099\n",
      "  train accuracy:\t\t90.03 %\n",
      "  validation accuracy:\t\t46.58 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 58 #количество проходов по данным\n",
    "\n",
    "batch_size = 150 #размер мини-батча\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t30.38 %\n",
      "Нужно больше магии!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 2\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 80:\n",
    "    print \"Achievement unlocked: колдун 80 уровня\"\n",
    "else:\n",
    "    print \"Нужно больше магии!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполните форму\n",
    "\n",
    "https://goo.gl/forms/FsANPB1jSqmX1JBJ3"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
